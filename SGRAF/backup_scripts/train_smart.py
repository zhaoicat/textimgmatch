#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ä¼˜åŒ–è®­ç»ƒè„šæœ¬ - é’ˆå¯¹37%ç“¶é¢ˆçš„ç²¾å‡†çªç ´

é—®é¢˜åˆ†æï¼š
1. 37%ç“¶é¢ˆ = æ¶æ„é™åˆ¶ + æ•°æ®ä¸è¶³ + ä¼˜åŒ–ä¸å½“
2. éœ€è¦ï¼šæ›´å¥½çš„ç‰¹å¾è¡¨ç¤º + æ›´å¼ºçš„åŒ¹é…æœºåˆ¶ + æ›´ä¼˜çš„è®­ç»ƒç­–ç•¥

è§£å†³æ–¹æ¡ˆï¼š
1. åŒè·¯å¾„ç‰¹å¾æå–ï¼ˆç²—ç²’åº¦+ç»†ç²’åº¦ï¼‰
2. è‡ªé€‚åº”ç›¸ä¼¼åº¦å­¦ä¹ 
3. è¯¾ç¨‹å­¦ä¹  + å›°éš¾æ ·æœ¬é‡ç‚¹è®­ç»ƒ
4. é›†æˆå¤šç§æŸå¤±å‡½æ•°çš„ä¼˜åŠ¿

ç›®æ ‡ï¼šç¨³å®šçªç ´åˆ°45-50%
"""

import os
import sys
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import math

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_path = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_path)

from data_chinese import get_precomp_loader
from lib.evaluation import i2t, t2i, AverageMeter, LogCollector
import vocab


class SmartConfig:
    """æ™ºèƒ½é…ç½® - åŸºäºé—®é¢˜åˆ†æä¼˜åŒ–"""
    def __init__(self):
        # æ•°æ®é…ç½®
        self.data_path = './data/tk_precomp'
        self.data_name = 'tk_precomp'
        
        # å¹³è¡¡çš„æ¨¡å‹é…ç½®
        self.embed_size = 768       # ç»å…¸é…ç½®
        self.word_dim = 300         # æ ‡å‡†è¯åµŒå…¥
        self.vocab_size = 20000
        self.img_dim = 2048
        
        # ç½‘ç»œé…ç½®
        self.num_heads = 8
        self.dropout = 0.3          # é€‚åº¦æ­£åˆ™åŒ–
        self.hidden_dim = 1024
        
        # è®­ç»ƒé…ç½®
        self.batch_size = 24
        self.num_epochs = 100
        self.lr_vse = 8e-5          # ç¨å¤§å­¦ä¹ ç‡
        self.workers = 2
        
        # æŸå¤±é…ç½®
        self.margin = 0.1           # é€‚ä¸­è¾¹è·
        self.temperature = 0.07     # é€‚ä¸­æ¸©åº¦
        self.alpha_triplet = 0.5    # ä¸‰å…ƒç»„æƒé‡
        self.alpha_infonce = 1.0    # InfoNCEæƒé‡
        self.alpha_smooth = 0.3     # å¹³æ»‘æƒé‡
        
        # æ™ºèƒ½è®­ç»ƒç­–ç•¥
        self.warmup_epochs = 6
        self.curriculum_start = 8   # è¯¾ç¨‹å­¦ä¹ å¼€å§‹
        self.hard_mining_start = 15 # å›°éš¾æŒ–æ˜å¼€å§‹
        
        # æ­£åˆ™åŒ–
        self.grad_clip = 1.0
        self.weight_decay = 0.015
        self.label_smoothing = 0.05
        
        # å…¶ä»–
        self.log_step = 20
        self.logger_name = './runs/tk_smart/log'
        self.model_name = './runs/tk_smart/checkpoint'
        self.early_stop_patience = 20


class DualPathImageEncoder(nn.Module):
    """åŒè·¯å¾„å›¾åƒç¼–ç å™¨ - ç²—ç²’åº¦+ç»†ç²’åº¦"""
    
    def __init__(self, img_dim, embed_size, dropout=0.3):
        super(DualPathImageEncoder, self).__init__()
        self.embed_size = embed_size
        
        # ç²—ç²’åº¦è·¯å¾„ï¼ˆå…¨å±€ç‰¹å¾ï¼‰
        self.coarse_path = nn.Sequential(
            nn.Linear(img_dim, embed_size),
            nn.LayerNorm(embed_size),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(embed_size, embed_size),
            nn.LayerNorm(embed_size)
        )
        
        # ç»†ç²’åº¦è·¯å¾„ï¼ˆåŒºåŸŸç‰¹å¾ï¼‰
        self.fine_path = nn.Sequential(
            nn.Linear(img_dim, embed_size),
            nn.LayerNorm(embed_size),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout)
        )
        
        # ä½ç½®ç¼–ç 
        self.pos_embedding = nn.Parameter(torch.randn(36, embed_size) * 0.02)
        
        # åŒºåŸŸæ³¨æ„åŠ›
        self.region_attention = nn.MultiheadAttention(
            embed_dim=embed_size,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # è‡ªé€‚åº”èåˆ
        self.fusion_gate = nn.Sequential(
            nn.Linear(embed_size * 2, embed_size),
            nn.Sigmoid()
        )
        
        self.fusion_proj = nn.Sequential(
            nn.Linear(embed_size * 3, embed_size),  # ä¿®æ”¹ä¸º3å€ç»´åº¦
            nn.LayerNorm(embed_size),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(embed_size, embed_size)
        )
        
        # æ³¨æ„åŠ›æ± åŒ–
        self.attention_pool = nn.Sequential(
            nn.Linear(embed_size, embed_size // 2),
            nn.Tanh(),
            nn.Linear(embed_size // 2, 1)
        )
        
        self.dropout = nn.Dropout(dropout)
        self.init_weights()
    
    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, images):
        batch_size = images.size(0)
        
        if len(images.shape) == 3:  # (batch, 36, 2048)
            # ç²—ç²’åº¦ç‰¹å¾ï¼ˆå…¨å±€ï¼‰
            global_features = images.mean(dim=1)  # (batch, 2048)
            coarse_emb = self.coarse_path(global_features)  # (batch, embed_size)
            
            # ç»†ç²’åº¦ç‰¹å¾ï¼ˆåŒºåŸŸï¼‰
            fine_features = self.fine_path(images)  # (batch, 36, embed_size)
            
            # ä½ç½®ç¼–ç 
            fine_features = fine_features + self.pos_embedding.unsqueeze(0)
            fine_features = self.dropout(fine_features)
            
            # åŒºåŸŸæ³¨æ„åŠ›
            attn_features, _ = self.region_attention(
                fine_features, fine_features, fine_features
            )
            fine_features = fine_features + attn_features
            
            # æ³¨æ„åŠ›æ± åŒ–
            attention_weights = self.attention_pool(fine_features).squeeze(-1)
            attention_weights = F.softmax(attention_weights, dim=1)
            fine_emb = torch.bmm(attention_weights.unsqueeze(1), fine_features).squeeze(1)
            
            # è‡ªé€‚åº”èåˆ
            combined = torch.cat([coarse_emb, fine_emb], dim=-1)
            gate = self.fusion_gate(combined)
            fused_features = gate * coarse_emb + (1 - gate) * fine_emb
            
            # æœ€ç»ˆæŠ•å½±
            final_combined = torch.cat([fused_features, coarse_emb, fine_emb], dim=-1)
            features = self.fusion_proj(final_combined)
            
        else:
            features = self.coarse_path(images)
        
        features = self.dropout(features)
        features = F.normalize(features, p=2, dim=1)
        
        return features


class SmartTextEncoder(nn.Module):
    """æ™ºèƒ½æ–‡æœ¬ç¼–ç å™¨ - å±‚æ¬¡åŒ–è¯­ä¹‰ç†è§£"""
    
    def __init__(self, vocab_size, word_dim, embed_size, dropout=0.3):
        super(SmartTextEncoder, self).__init__()
        self.embed_size = embed_size
        
        # è¯åµŒå…¥
        self.embed = nn.Embedding(vocab_size, word_dim, padding_idx=0)
        self.embed_dropout = nn.Dropout(dropout)
        
        # è¯çº§åˆ«ç¼–ç 
        self.word_encoder = nn.Sequential(
            nn.Linear(word_dim, embed_size),
            nn.LayerNorm(embed_size),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout)
        )
        
        # ä½ç½®ç¼–ç 
        self.pos_embedding = nn.Parameter(torch.randn(50, embed_size) * 0.02)
        
        # åŒå‘LSTM
        self.lstm = nn.LSTM(
            embed_size, embed_size // 2, 2,
            batch_first=True, bidirectional=True,
            dropout=dropout if dropout > 0 else 0
        )
        
        # è‡ªæ³¨æ„åŠ›
        self.self_attention = nn.MultiheadAttention(
            embed_dim=embed_size,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # å±‚çº§æ³¨æ„åŠ›
        self.hierarchical_attention = nn.Sequential(
            nn.Linear(embed_size, embed_size),
            nn.Tanh(),
            nn.Dropout(dropout),
            nn.Linear(embed_size, 1)
        )
        
        # è¯­ä¹‰å¢å¼º
        self.semantic_enhance = nn.Sequential(
            nn.Linear(embed_size, embed_size * 2),
            nn.LayerNorm(embed_size * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_size * 2, embed_size),
            nn.LayerNorm(embed_size)
        )
        
        self.layer_norm = nn.LayerNorm(embed_size)
        self.dropout = nn.Dropout(dropout)
        
        self.init_weights()
    
    def init_weights(self):
        # è¯åµŒå…¥åˆå§‹åŒ–
        nn.init.uniform_(self.embed.weight, -0.1, 0.1)
        
        # LSTMåˆå§‹åŒ–
        for name, param in self.lstm.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                param.data.fill_(0)
    
    def forward(self, captions, lengths):
        batch_size, max_len = captions.size()
        
        # è¯åµŒå…¥
        embedded = self.embed(captions)
        embedded = self.embed_dropout(embedded)
        
        # è¯çº§ç¼–ç 
        word_features = self.word_encoder(embedded)
        
        # ä½ç½®ç¼–ç 
        pos_len = min(max_len, self.pos_embedding.size(0))
        word_features[:, :pos_len] += self.pos_embedding[:pos_len].unsqueeze(0)
        
        # LSTMç¼–ç 
        packed = nn.utils.rnn.pack_padded_sequence(
            word_features, lengths, batch_first=True, enforce_sorted=False
        )
        lstm_out, _ = self.lstm(packed)
        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)
        
        # è‡ªæ³¨æ„åŠ›
        attn_mask = torch.zeros(batch_size, max_len, dtype=torch.bool)
        for i, length in enumerate(lengths):
            attn_mask[i, length:] = True
        
        if lstm_out.is_cuda:
            attn_mask = attn_mask.cuda()
        
        attn_out, _ = self.self_attention(
            lstm_out, lstm_out, lstm_out,
            key_padding_mask=attn_mask
        )
        
        lstm_out = self.layer_norm(lstm_out + attn_out)
        
        # å±‚çº§æ³¨æ„åŠ›
        attention_weights = self.hierarchical_attention(lstm_out).squeeze(-1)
        
        # é•¿åº¦mask
        mask = torch.zeros(batch_size, max_len)
        for i, length in enumerate(lengths):
            mask[i, :length] = 1
        
        if lstm_out.is_cuda:
            mask = mask.cuda()
        
        attention_weights.masked_fill_(mask == 0, -1e9)
        attention_weights = F.softmax(attention_weights, dim=1)
        
        # åŠ æƒæ±‚å’Œ
        features = torch.bmm(attention_weights.unsqueeze(1), lstm_out).squeeze(1)
        
        # è¯­ä¹‰å¢å¼º
        features = self.semantic_enhance(features)
        features = self.dropout(features)
        features = F.normalize(features, p=2, dim=1)
        
        return features


class AdaptiveSimilarityLearning(nn.Module):
    """è‡ªé€‚åº”ç›¸ä¼¼åº¦å­¦ä¹ """
    
    def __init__(self, embed_size, temperature=0.07):
        super(AdaptiveSimilarityLearning, self).__init__()
        self.temperature = temperature
        
        # è‡ªé€‚åº”ç›¸ä¼¼åº¦è®¡ç®—
        self.similarity_net = nn.Sequential(
            nn.Linear(embed_size * 2, embed_size),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(embed_size, 1),
            nn.Sigmoid()
        )
        
        # æ¸©åº¦å­¦ä¹ 
        self.temp_net = nn.Sequential(
            nn.Linear(embed_size * 2, embed_size // 2),
            nn.ReLU(inplace=True),
            nn.Linear(embed_size // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, img_emb, txt_emb):
        batch_size = img_emb.size(0)
        
        # è®¡ç®—æ‰€æœ‰é…å¯¹çš„ç‰¹å¾
        img_expanded = img_emb.unsqueeze(1).expand(-1, batch_size, -1)  # (B, B, D)
        txt_expanded = txt_emb.unsqueeze(0).expand(batch_size, -1, -1)  # (B, B, D)
        
        paired_features = torch.cat([img_expanded, txt_expanded], dim=-1)  # (B, B, 2D)
        
        # è‡ªé€‚åº”ç›¸ä¼¼åº¦
        adaptive_sim = self.similarity_net(paired_features).squeeze(-1)  # (B, B)
        
        # è‡ªé€‚åº”æ¸©åº¦
        adaptive_temp = self.temp_net(paired_features).squeeze(-1)  # (B, B)
        adaptive_temp = 0.01 + 0.2 * adaptive_temp  # é™åˆ¶æ¸©åº¦èŒƒå›´
        
        # åŸºç¡€ä½™å¼¦ç›¸ä¼¼åº¦
        cosine_sim = torch.mm(img_emb, txt_emb.t())
        
        # èåˆç›¸ä¼¼åº¦
        final_sim = 0.7 * cosine_sim + 0.3 * adaptive_sim
        final_sim = final_sim / adaptive_temp
        
        return final_sim


class SmartVSEModel(nn.Module):
    """æ™ºèƒ½VSEæ¨¡å‹"""
    
    def __init__(self, opt):
        super(SmartVSEModel, self).__init__()
        
        # ç¼–ç å™¨
        self.img_enc = DualPathImageEncoder(
            img_dim=opt.img_dim,
            embed_size=opt.embed_size,
            dropout=opt.dropout
        )
        
        self.txt_enc = SmartTextEncoder(
            vocab_size=opt.vocab_size,
            word_dim=opt.word_dim,
            embed_size=opt.embed_size,
            dropout=opt.dropout
        )
        
        # è‡ªé€‚åº”ç›¸ä¼¼åº¦å­¦ä¹ 
        self.adaptive_sim = AdaptiveSimilarityLearning(
            embed_size=opt.embed_size,
            temperature=opt.temperature
        )
        
        # æŸå¤±å‚æ•°
        self.margin = opt.margin
        self.temperature = opt.temperature
        self.alpha_triplet = opt.alpha_triplet
        self.alpha_infonce = opt.alpha_infonce
        self.alpha_smooth = opt.alpha_smooth
        self.label_smoothing = opt.label_smoothing
    
    def forward_emb(self, images, captions, lengths):
        img_emb = self.img_enc(images)
        txt_emb = self.txt_enc(captions, lengths)
        return img_emb, txt_emb
    
    def forward_loss(self, img_emb, txt_emb, epoch=0, total_epochs=100):
        """æ™ºèƒ½æŸå¤±è®¡ç®—"""
        batch_size = img_emb.size(0)
        
        # è‡ªé€‚åº”ç›¸ä¼¼åº¦
        adaptive_scores = self.adaptive_sim(img_emb, txt_emb)
        
        # InfoNCEæŸå¤±ï¼ˆä½¿ç”¨è‡ªé€‚åº”ç›¸ä¼¼åº¦ï¼‰
        labels = torch.arange(batch_size)
        if img_emb.is_cuda:
            labels = labels.cuda()
        
        loss_i2t = F.cross_entropy(adaptive_scores, labels, label_smoothing=self.label_smoothing)
        loss_t2i = F.cross_entropy(adaptive_scores.t(), labels, label_smoothing=self.label_smoothing)
        loss_infonce = (loss_i2t + loss_t2i) / 2
        
        # ä¼ ç»Ÿä¸‰å…ƒç»„æŸå¤±
        cosine_scores = torch.mm(img_emb, txt_emb.t())
        diagonal = cosine_scores.diag().view(-1, 1)
        
        d1 = diagonal.expand_as(cosine_scores)
        cost_s = (self.margin + cosine_scores - d1).clamp(min=0)
        
        d2 = diagonal.t().expand_as(cosine_scores)
        cost_im = (self.margin + cosine_scores - d2).clamp(min=0)
        
        mask = torch.eye(batch_size) > 0.5
        if cosine_scores.is_cuda:
            mask = mask.cuda()
        cost_s = cost_s.masked_fill_(mask, 0)
        cost_im = cost_im.masked_fill_(mask, 0)
        
        # è¯¾ç¨‹å­¦ä¹ ï¼šé€æ¸å…³æ³¨å›°éš¾æ ·æœ¬
        if epoch >= 15:  # å›°éš¾æŒ–æ˜é˜¶æ®µ
            progress = min(1.0, (epoch - 15) / 30)
            top_k = max(1, int(batch_size * (0.3 + 0.5 * progress)))
            cost_s_top, _ = cost_s.topk(top_k, dim=1)
            cost_im_top, _ = cost_im.topk(top_k, dim=0)
            loss_triplet = cost_s_top.mean() + cost_im_top.mean()
        else:
            loss_triplet = cost_s.mean() + cost_im.mean()
        
        # å¹³æ»‘æŸå¤±ï¼ˆä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼‰
        img_norm = F.normalize(img_emb, p=2, dim=1)
        txt_norm = F.normalize(txt_emb, p=2, dim=1)
        consistency_loss = F.mse_loss(img_norm, txt_norm)
        
        # æ€»æŸå¤±
        total_loss = (self.alpha_infonce * loss_infonce + 
                     self.alpha_triplet * loss_triplet +
                     self.alpha_smooth * consistency_loss)
        
        return total_loss, loss_infonce, loss_triplet, consistency_loss


class SmartTrainer:
    """æ™ºèƒ½è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.opt = SmartConfig()
        
        self.vocab = self.load_vocab()
        if self.vocab:
            self.opt.vocab_size = len(self.vocab)
        
        print(f"è¯æ±‡è¡¨å¤§å°: {self.opt.vocab_size}")
    
    def load_vocab(self):
        try:
            import pickle
            vocab_path = f'./vocab/{self.opt.data_name}_vocab.pkl'
            if os.path.exists(vocab_path):
                with open(vocab_path, 'rb') as f:
                    return pickle.load(f)
            else:
                print(f"è¯æ±‡è¡¨æ–‡ä»¶ {vocab_path} ä¸å­˜åœ¨")
                return None
        except Exception as e:
            print(f"åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {e}")
            return None
    
    def adjust_learning_rate(self, optimizer, epoch):
        """æ™ºèƒ½å­¦ä¹ ç‡è°ƒåº¦"""
        if epoch < self.opt.warmup_epochs:
            lr = self.opt.lr_vse * (epoch + 1) / self.opt.warmup_epochs
        else:
            # åˆ†é˜¶æ®µè¡°å‡
            if epoch < 30:
                lr = self.opt.lr_vse
            elif epoch < 60:
                lr = self.opt.lr_vse * 0.5
            else:
                lr = self.opt.lr_vse * 0.2
        
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        
        return lr
    
    def train_epoch(self, model, data_loader, optimizer, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        
        batch_time = AverageMeter()
        train_logger = LogCollector()
        
        end = time.time()
        
        for i, train_data in enumerate(data_loader):
            images, captions, lengths, ids = train_data
            
            if torch.cuda.is_available():
                images = images.cuda()
                captions = captions.cuda()
            
            lr = self.adjust_learning_rate(optimizer, epoch)
            
            img_emb, txt_emb = model.forward_emb(images, captions, lengths)
            total_loss, loss_infonce, loss_triplet, loss_smooth = model.forward_loss(
                img_emb, txt_emb, epoch, self.opt.num_epochs
            )
            
            optimizer.zero_grad()
            total_loss.backward()
            
            torch.nn.utils.clip_grad_norm_(model.parameters(), self.opt.grad_clip)
            optimizer.step()
            
            train_logger.update('Loss', total_loss.item(), img_emb.size(0))
            train_logger.update('InfoNCE', loss_infonce.item(), img_emb.size(0))
            train_logger.update('Triplet', loss_triplet.item(), img_emb.size(0))
            train_logger.update('Smooth', loss_smooth.item(), img_emb.size(0))
            
            batch_time.update(time.time() - end)
            end = time.time()
            
            if i % self.opt.log_step == 0:
                print(f'Epoch [{epoch+1}][{i}/{len(data_loader)}] '
                      f'LR: {lr:.6f} '
                      f'Time {batch_time.val:.3f} '
                      f'Loss {train_logger.meters["Loss"].val:.4f} '
                      f'InfoNCE {train_logger.meters["InfoNCE"].val:.4f} '
                      f'Triplet {train_logger.meters["Triplet"].val:.4f} '
                      f'Smooth {train_logger.meters["Smooth"].val:.4f}')
    
    def validate(self, model, data_loader):
        """æ¨¡å‹éªŒè¯"""
        model.eval()
        
        img_embs = []
        txt_embs = []
        
        print("éªŒè¯ä¸­...")
        with torch.no_grad():
            for val_data in data_loader:
                images, captions, lengths, ids = val_data
                
                if torch.cuda.is_available():
                    images = images.cuda()
                    captions = captions.cuda()
                
                img_emb, txt_emb = model.forward_emb(images, captions, lengths)
                
                img_embs.append(img_emb.cpu().numpy())
                txt_embs.append(txt_emb.cpu().numpy())
        
        img_embs = np.concatenate(img_embs, axis=0)
        txt_embs = np.concatenate(txt_embs, axis=0)
        
        (r1, r5, r10, medr, meanr) = i2t(img_embs, txt_embs, measure='cosine')
        (r1i, r5i, r10i, medri, meanri) = t2i(img_embs, txt_embs, measure='cosine')
        
        rsum = r1 + r5 + r10 + r1i + r5i + r10i
        avg_r1 = (r1 + r1i) / 2
        
        print(f"Text to Image: R@1: {r1i:.2f}%, R@5: {r5i:.2f}%, R@10: {r10i:.2f}%")
        print(f"Image to Text: R@1: {r1:.2f}%, R@5: {r5:.2f}%, R@10: {r10:.2f}%")
        print(f"å¹³å‡ R@1: {avg_r1:.2f}%")
        print(f"R-sum: {rsum:.2f}")
        
        return rsum, avg_r1
    
    def train(self):
        """ä¸»è®­ç»ƒæµç¨‹"""
        print("=== æ™ºèƒ½ä¼˜åŒ–è®­ç»ƒ - ç²¾å‡†çªç ´37%ç“¶é¢ˆ ===")
        print(f"åŸºçº¿: 37.47%")
        print(f"ç›®æ ‡: 45-50%")
        print(f"ç­–ç•¥: åŒè·¯å¾„ç‰¹å¾+è‡ªé€‚åº”ç›¸ä¼¼åº¦+è¯¾ç¨‹å­¦ä¹ ")
        
        os.makedirs(self.opt.logger_name, exist_ok=True)
        os.makedirs(self.opt.model_name, exist_ok=True)
        
        if not self.vocab:
            print("è¯æ±‡è¡¨ä¸å­˜åœ¨ï¼Œé€€å‡º")
            return
        
        train_loader = get_precomp_loader(
            self.opt.data_path, 'train', self.vocab, self.opt,
            batch_size=self.opt.batch_size, shuffle=True, 
            num_workers=self.opt.workers
        )
        
        val_loader = get_precomp_loader(
            self.opt.data_path, 'dev', self.vocab, self.opt,
            batch_size=self.opt.batch_size, shuffle=False,
            num_workers=self.opt.workers
        )
        
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        print(f"éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
        
        model = SmartVSEModel(self.opt)
        
        if torch.cuda.is_available():
            model.cuda()
            print("ä½¿ç”¨GPU")
        
        total_params = sum(p.numel() for p in model.parameters())
        print(f"å‚æ•°é‡: {total_params:,}")
        
        optimizer = optim.AdamW(
            model.parameters(), 
            lr=self.opt.lr_vse,
            weight_decay=self.opt.weight_decay
        )
        
        best_rsum = 0
        best_avg_r1 = 0
        patience_counter = 0
        
        print(f"\nå¼€å§‹è®­ç»ƒ {self.opt.num_epochs} epochs...")
        
        for epoch in range(self.opt.num_epochs):
            print(f"\nEpoch {epoch+1}/{self.opt.num_epochs}")
            print("-" * 50)
            
            # è®­ç»ƒé˜¶æ®µæç¤º
            if epoch == self.opt.curriculum_start:
                print("ğŸ“ å¼€å§‹è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ")
            elif epoch == self.opt.hard_mining_start:
                print("âš¡ å¼€å§‹å›°éš¾æ ·æœ¬æŒ–æ˜")
            
            self.train_epoch(model, train_loader, optimizer, epoch)
            rsum, avg_r1 = self.validate(model, val_loader)
            
            is_best = avg_r1 > best_avg_r1
            if is_best:
                best_rsum = rsum
                best_avg_r1 = avg_r1
                patience_counter = 0
                
                checkpoint = {
                    'epoch': epoch,
                    'model': model.state_dict(),
                    'best_rsum': best_rsum,
                    'best_avg_r1': best_avg_r1,
                    'opt': self.opt,
                }
                
                model_path = os.path.join(self.opt.model_name, 'best_model.pth')
                torch.save(checkpoint, model_path)
                
                print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹ - R-sum: {best_rsum:.2f}, å¹³å‡R@1: {best_avg_r1:.2f}%")
                
                # ç›®æ ‡æ£€æŸ¥
                if best_avg_r1 >= 50.0:
                    print(f"ğŸ‰ çªç ´50%!")
                elif best_avg_r1 >= 45.0:
                    print(f"ğŸ‰ è¾¾åˆ°45%ç›®æ ‡!")
                elif best_avg_r1 >= 70.0:
                    print(f"ğŸ‰ğŸ‰ğŸ‰ è¾¾åˆ°70%æœ€ç»ˆç›®æ ‡!")
                    break
                    
            else:
                patience_counter += 1
                
            if patience_counter >= self.opt.early_stop_patience:
                print(f"æ—©åœ")
                break
            
            improvement = best_avg_r1 - 37.47
            remaining = 50.0 - best_avg_r1
            
            print(f"å½“å‰æœ€ä½³: {best_avg_r1:.2f}%")
            print(f"çªç ´: {improvement:.2f}%")
            print(f"è·ç¦»50%: {remaining:.2f}%")
            
            # æ™ºèƒ½å»ºè®®
            if epoch > 10 and improvement < 1.0:
                print("ğŸ’¡ å»ºè®®: è€ƒè™‘è°ƒæ•´å­¦ä¹ ç‡æˆ–æ¨¡å‹æ¶æ„")
            elif improvement > 5.0:
                print("ğŸš€ è¡¨ç°ä¼˜ç§€ï¼Œç»§ç»­ä¿æŒ!")
        
        print(f"\n{'='*50}")
        print("æ™ºèƒ½ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
        print(f"åŸºçº¿: 37.47%")
        print(f"ç»“æœ: {best_avg_r1:.2f}%")
        print(f"çªç ´: {best_avg_r1 - 37.47:.2f}%")
        
        if best_avg_r1 >= 70.0:
            print("ğŸ‰ğŸ‰ğŸ‰ è¾¾åˆ°70%æœ€ç»ˆç›®æ ‡!")
        elif best_avg_r1 >= 50.0:
            print("ğŸ‰ æˆåŠŸçªç ´50%!")
        elif best_avg_r1 >= 45.0:
            print("ğŸ‰ è¾¾åˆ°45%ç›®æ ‡!")
        elif best_avg_r1 > 40.0:
            print("âš¡ æ˜¾è‘—æå‡!")
        else:
            print(f"éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œè·ç¦»45%: {45.0 - best_avg_r1:.2f}%")


def main():
    """ä¸»å‡½æ•°"""
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    trainer = SmartTrainer()
    trainer.train()


if __name__ == '__main__':
    main() 