#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import shutil
import pickle
import logging
import argparse
import numpy as np
from collections import OrderedDict

import torch
import torch.nn as nn
import torch.nn.init
import torch.backends.cudnn as cudnn
from torch.utils.data import DataLoader
import torch.nn.functional as F

import data
from model import SGRAF
from evaluation import i2t, t2i, AverageMeter, LogCollector, encode_data, shard_attn_scores

def main():
    # è¶…å‚æ•°è®¾ç½®
    opt = argparse.Namespace()
    opt.data_path = './data/'
    opt.data_name = 'tk_precomp'
    opt.vocab_path = './vocab/'
    opt.margin = 0.2
    opt.num_epochs = 80
    opt.batch_size = 128
    opt.word_dim = 300
    opt.embed_size = 1024
    opt.grad_clip = 2.0
    opt.crop_size = 224
    opt.num_layers = 1
    opt.learning_rate = 0.0002
    opt.lr_update = 15
    opt.workers = 10
    opt.log_step = 10
    opt.val_step = 500
    opt.logger_name = './runs/simple_effective'
    opt.model_name = './runs/simple_effective'
    opt.resume = ''
    opt.max_violation = True
    opt.img_dim = 2048
    opt.measure = 'cosine'
    opt.use_abs = False
    opt.no_imgnorm = False
    opt.reset_train = True
    opt.raw_feature_norm = "clipped_l2norm"
    opt.agg_func = "LogSumExp"
    opt.cross_attn = "t2i"
    opt.precomp_enc_type = "basic"
    opt.bi_gru = False
    opt.lambda_lse = 6.0
    opt.lambda_softmax = 9.0
    opt.no_txtnorm = False
    opt.sim_dim = 256
    opt.module_name = 'SGR'
    opt.sgr_step = 3
    
    print("=== ç®€å•æœ‰æ•ˆè®­ç»ƒ - ä¸“æ³¨åŸºç¡€ä¼˜åŒ– ===")
    print(f"åŸºçº¿: 37.47%")
    print(f"ç›®æ ‡: 40-42%")
    print(f"ç­–ç•¥: å­¦ä¹ ç‡è°ƒä¼˜+æ•°æ®å¢å¼º+æŸå¤±å‡½æ•°æ”¹è¿›")
    
    logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)
    tb_logger = LogCollector()

    # åŠ è½½è¯æ±‡è¡¨
    with open(os.path.join(opt.vocab_path, '%s_vocab.pkl' % opt.data_name), 'rb') as f:
        vocab = pickle.load(f)
    opt.vocab_size = len(vocab)
    print(f"è¯æ±‡è¡¨å¤§å°: {opt.vocab_size}")

    # æ„å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = data.get_loaders(
        opt.data_name, vocab, opt.batch_size, opt.workers, opt)

    print(f"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
    print(f"éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")

    # æ„å»ºæ¨¡å‹
    model = SGRAF(opt)

    # ä½¿ç”¨GPU
    if torch.cuda.is_available():
        print("ä½¿ç”¨GPU")
    else:
        print("ä½¿ç”¨CPU")

    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"å‚æ•°é‡: {total_params:,}")

    # è®­ç»ƒ
    print(f"å¼€å§‹è®­ç»ƒ {opt.num_epochs} epochs...")
    
    best_rsum = 0
    best_r1 = 0
    
    for epoch in range(opt.num_epochs):
        print(f"\nEpoch {epoch+1}/{opt.num_epochs}")
        print("-" * 50)
        
        # è°ƒæ•´å­¦ä¹ ç‡
        adjust_learning_rate(opt, model.optimizer, epoch)
        
        # è®­ç»ƒä¸€ä¸ªepoch
        train_epoch(model, train_loader, epoch, opt)
        
        # éªŒè¯
        print("éªŒè¯ä¸­...")
        rsum, r1_avg = validate(model, val_loader, opt)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        is_best = rsum > best_rsum
        if is_best:
            best_rsum = rsum
            best_r1 = r1_avg
            save_checkpoint({
                'epoch': epoch + 1,
                'model': model.state_dict(),
                'best_rsum': best_rsum,
                'opt': opt,
                'Eiters': model.Eiters,
            }, is_best, filename='checkpoint_{}.pth.tar'.format(epoch), prefix=opt.model_name + '/')
            print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹ - R-sum: {rsum:.2f}, å¹³å‡R@1: {r1_avg:.2f}%")
        
        print(f"å½“å‰æœ€ä½³: {best_r1:.2f}%")
        improvement = best_r1 - 37.47
        print(f"æ”¹è¿›: {improvement:+.2f}%")
        print(f"è·ç¦»42%: {42 - best_r1:.2f}%")
        
        # æ—©åœæ£€æŸ¥
        if epoch > 30 and improvement < -2:
            print("æ€§èƒ½ä¸‹é™è¿‡å¤šï¼Œæ—©åœ")
            break

    print("\n" + "=" * 50)
    print("ç®€å•æœ‰æ•ˆè®­ç»ƒå®Œæˆ!")
    print(f"åŸºçº¿: 37.47%")
    print(f"ç»“æœ: {best_r1:.2f}%")
    print(f"æ”¹è¿›: {improvement:+.2f}%")
    if best_r1 >= 40:
        print("ğŸ‰ æˆåŠŸçªç ´40%!")
    else:
        print(f"ç»§ç»­åŠªåŠ›ï¼Œè·ç¦»40%è¿˜å·®: {40 - best_r1:.2f}%")

def train_epoch(model, train_loader, epoch, opt):
    # è®­ç»ƒæ¨¡å¼
    batch_time = AverageMeter()
    data_time = AverageMeter()
    train_logger = LogCollector()

    end = time.time()
    for i, train_data in enumerate(train_loader):
        # è®­ç»ƒæ¨¡å¼
        model.train_start()
        
        # æ•°æ®åŠ è½½æ—¶é—´
        data_time.update(time.time() - end)

        # è®¾ç½®logger
        model.logger = train_logger

        # æ›´æ–°æ¨¡å‹
        model.train_emb(*train_data)

        # è®°å½•æ—¶é—´
        batch_time.update(time.time() - end)
        end = time.time()

        # æ‰“å°è®­ç»ƒçŠ¶æ€
        if model.Eiters % opt.log_step == 0:
            print(f'Epoch [{epoch}][{i}/{len(train_loader)}] '
                  f'Time {batch_time.val:.3f} '
                  f'{str(model.logger)}')

def validate(model, val_loader, opt):
    # éªŒè¯æ¨¡å¼
    model.eval()
    
    with torch.no_grad():
        # ç¼–ç æ‰€æœ‰å›¾åƒå’Œæ–‡æœ¬
        img_embs, cap_embs, cap_lens = encode_data(model, val_loader, opt.log_step, logging.info)
        
        img_embs = np.array([img_embs[i] for i in range(0, len(img_embs), 5)])

        start = time.time()
        sims = shard_attn_scores(model, img_embs, cap_embs, cap_lens, opt, shard_size=128)
        end = time.time()
        print("è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µæ—¶é—´: {:.3f}s".format(end-start))

        # æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢
        (r1, r5, r10, medr, meanr) = i2t(img_embs, cap_embs, cap_lens, sims)
        print("Text to Image: R@1: {:.2f}%, R@5: {:.2f}%, R@10: {:.2f}%".format(r1, r5, r10))
        
        # å›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢
        (r1i, r5i, r10i, medri, meanri) = t2i(img_embs, cap_embs, cap_lens, sims)
        print("Image to Text: R@1: {:.2f}%, R@5: {:.2f}%, R@10: {:.2f}%".format(r1i, r5i, r10i))
        
        # è®¡ç®—æ€»åˆ†å’Œå¹³å‡R@1
        rsum = r1 + r5 + r10 + r1i + r5i + r10i
        r1_avg = (r1 + r1i) / 2
        print("å¹³å‡ R@1: {:.2f}%".format(r1_avg))
        print("R-sum: {:.2f}".format(rsum))

    return rsum, r1_avg

def save_checkpoint(state, is_best, filename='checkpoint.pth.tar', prefix=''):
    tries = 15
    error = None

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not os.path.exists(prefix):
        os.makedirs(prefix)

    # å°è¯•ä¿å­˜
    while tries:
        try:
            torch.save(state, prefix + filename)
            if is_best:
                shutil.copyfile(prefix + filename, prefix + 'model_best.pth.tar')
        except IOError as e:
            error = e
            tries -= 1
        else:
            break
        print('æ¨¡å‹ä¿å­˜ {} å¤±è´¥, å‰©ä½™å°è¯•æ¬¡æ•° {}'.format(filename, tries))
        if not tries:
            raise error

def adjust_learning_rate(opt, optimizer, epoch):
    """
    Sets the learning rate to the initial LR
    decayed by 10 after opt.lr_update epoch
    """
    lr = opt.learning_rate * (0.1 ** (epoch // opt.lr_update))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

if __name__ == '__main__':
    main() 