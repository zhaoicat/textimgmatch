#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¶…çº§ä¼˜åŒ–è®­ç»ƒè„šæœ¬ - å†²å‡»70% R@1ç›®æ ‡

åŸºäºå‰æ¬¡æˆåŠŸç»éªŒï¼ˆR@1ä»0%æå‡åˆ°37.47%ï¼‰ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–ï¼š
1. å¢å¤§æ¨¡å‹å®¹é‡ï¼ˆæ›´å¤§åµŒå…¥ç»´åº¦ã€æ›´æ·±ç½‘ç»œï¼‰
2. æ›´å¼ºçš„æ•°æ®å¢å¼ºå’Œè¯¾ç¨‹å­¦ä¹ 
3. é›†æˆå¤šç§æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–æŠ€æœ¯
4. è‡ªé€‚åº”è®­ç»ƒç­–ç•¥å’Œå¤šå°ºåº¦ç‰¹å¾
5. çŸ¥è¯†è’¸é¦å’Œæ¨¡å‹é›†æˆæ€æƒ³
"""

import os
import sys
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import math
from collections import defaultdict

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_path = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_path)

from data_chinese import get_precomp_loader
from lib.evaluation import i2t, t2i, AverageMeter, LogCollector
import vocab


class SuperOptimizedConfig:
    """è¶…çº§ä¼˜åŒ–é…ç½® - å†²å‡»70%"""
    def __init__(self):
        # æ•°æ®é…ç½®
        self.data_path = './data/tk_precomp'
        self.data_name = 'tk_precomp'
        
        # æ¨¡å‹é…ç½® - å¤§å¹…å¢å¼º
        self.embed_size = 1024  # å¢å¤§åµŒå…¥ç»´åº¦
        self.word_dim = 512     # å¢å¤§è¯åµŒå…¥ç»´åº¦
        self.vocab_size = 20000
        self.img_dim = 2048
        
        # æ·±åº¦ç½‘ç»œé…ç½®
        self.num_heads = 16      # æ›´å¤šæ³¨æ„åŠ›å¤´
        self.num_layers = 4      # æ›´æ·±çš„ç½‘ç»œ
        self.dropout = 0.15      # ç¨å¾®å¢åŠ dropouté˜²è¿‡æ‹Ÿåˆ
        self.hidden_dim = 2048   # æ›´å¤§çš„éšè—å±‚
        
        # è®­ç»ƒé…ç½® - ç²¾ç»†è°ƒæ•´
        self.batch_size = 24     # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°
        self.num_epochs = 80     # æ›´å¤šè®­ç»ƒè½®æ¬¡
        self.lr_vse = 0.0001     # æ›´å°çš„å­¦ä¹ ç‡ç²¾ç»†è®­ç»ƒ
        self.lr_warmup = 0.000005
        self.workers = 2
        
        # å¤šæŸå¤±é…ç½®
        self.margin = 0.15       # æ›´å°çš„è¾¹è·ï¼Œè¦æ±‚æ›´ç²¾ç¡®
        self.temperature = 0.05  # æ›´ä½æ¸©åº¦
        self.lambda_triplet = 1.0
        self.lambda_infonce = 1.0
        self.lambda_consistency = 0.3  # ä¸€è‡´æ€§æŸå¤±
        self.lambda_diversity = 0.2    # å¤šæ ·æ€§æŸå¤±
        
        # è¯¾ç¨‹å­¦ä¹ é…ç½®
        self.curriculum_epochs = 15
        self.hard_negative_start = 0.2
        self.hard_negative_end = 0.8
        
        # è®­ç»ƒç­–ç•¥
        self.warmup_epochs = 5
        self.cosine_restart_epochs = 20  # ä½™å¼¦é‡å¯
        self.grad_clip = 1.0
        self.ema_decay = 0.999   # æŒ‡æ•°ç§»åŠ¨å¹³å‡
        
        # æ•°æ®å¢å¼º
        self.mixup_alpha = 0.2   # Mixupå¢å¼º
        self.cutmix_alpha = 0.2  # CutMixå¢å¼º
        self.augment_prob = 0.3
        
        # å…¶ä»–é…ç½®
        self.log_step = 30
        self.val_step = 100
        self.logger_name = './runs/tk_super_optimized/log'
        self.model_name = './runs/tk_super_optimized/checkpoint'
        
        # æ—©åœé…ç½®
        self.early_stop_patience = 15
        self.target_performance = 70.0


class TransformerBlock(nn.Module):
    """Transformerå—"""
    
    def __init__(self, embed_size, num_heads, hidden_dim, dropout=0.1):
        super(TransformerBlock, self).__init__()
        
        self.attention = nn.MultiheadAttention(
            embed_dim=embed_size, 
            num_heads=num_heads, 
            dropout=dropout,
            batch_first=True
        )
        
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, embed_size),
            nn.Dropout(dropout)
        )
        
    def forward(self, x, mask=None):
        # Self-attention with residual connection
        attn_out, _ = self.attention(x, x, x, key_padding_mask=mask)
        x = self.norm1(x + attn_out)
        
        # Feed-forward with residual connection
        ff_out = self.feed_forward(x)
        x = self.norm2(x + ff_out)
        
        return x


class SuperImageEncoder(nn.Module):
    """è¶…çº§å›¾åƒç¼–ç å™¨ - å¤šå°ºåº¦+æ·±åº¦ç½‘ç»œ"""
    
    def __init__(self, img_dim, embed_size, num_heads=16, num_layers=4, hidden_dim=2048, dropout=0.15):
        super(SuperImageEncoder, self).__init__()
        self.embed_size = embed_size
        
        # å¤šå°ºåº¦ç‰¹å¾æŠ•å½±
        self.region_proj = nn.Linear(img_dim, embed_size)
        self.global_proj = nn.Linear(img_dim, embed_size)
        
        # ä½ç½®ç¼–ç 
        self.pos_embedding = nn.Parameter(torch.randn(36, embed_size))
        
        # æ·±åº¦Transformerç¼–ç å™¨
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(embed_size, num_heads, hidden_dim, dropout)
            for _ in range(num_layers)
        ])
        
        # å¤šå°ºåº¦èåˆ
        self.scale_attention = nn.Sequential(
            nn.Linear(embed_size * 2, embed_size),
            nn.Tanh(),
            nn.Linear(embed_size, 2),
            nn.Softmax(dim=-1)
        )
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Sequential(
            nn.Linear(embed_size, embed_size),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_size, embed_size)
        )
        
        self.dropout = nn.Dropout(dropout)
        self.init_weights()
    
    def init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, images):
        batch_size = images.size(0)
        
        if len(images.shape) == 3:  # (batch, regions, features)
            # åŒºåŸŸç‰¹å¾å¤„ç†
            regions = self.region_proj(images)  # (batch, 36, embed_size)
            
            # æ·»åŠ ä½ç½®ç¼–ç 
            regions = regions + self.pos_embedding.unsqueeze(0)
            regions = self.dropout(regions)
            
            # é€šè¿‡Transformerå±‚
            for transformer in self.transformer_blocks:
                regions = transformer(regions)
            
            # å…¨å±€ç‰¹å¾
            global_feat = self.global_proj(images.mean(dim=1))  # (batch, embed_size)
            region_feat = regions.mean(dim=1)  # (batch, embed_size)
            
            # å¤šå°ºåº¦èåˆ
            combined = torch.cat([global_feat, region_feat], dim=-1)
            scale_weights = self.scale_attention(combined)  # (batch, 2)
            
            features = (scale_weights[:, 0:1] * global_feat + 
                       scale_weights[:, 1:2] * region_feat)
        else:
            features = self.region_proj(images)
        
        # è¾“å‡ºæŠ•å½±
        features = self.output_proj(features)
        features = self.dropout(features)
        
        # L2æ ‡å‡†åŒ–
        features = F.normalize(features, p=2, dim=1)
        
        return features


class SuperTextEncoder(nn.Module):
    """è¶…çº§æ–‡æœ¬ç¼–ç å™¨ - åˆ†å±‚å¤„ç†+æ³¨æ„åŠ›"""
    
    def __init__(self, vocab_size, word_dim, embed_size, num_layers=4, dropout=0.15):
        super(SuperTextEncoder, self).__init__()
        self.embed_size = embed_size
        self.word_dim = word_dim
        
        # è¯åµŒå…¥å±‚ - æ›´å¤§ç»´åº¦
        self.embed = nn.Embedding(vocab_size, word_dim)
        self.embed_dropout = nn.Dropout(dropout)
        
        # ä½ç½®ç¼–ç 
        self.pos_embedding = nn.Parameter(torch.randn(100, word_dim))  # æ”¯æŒæœ€é•¿100è¯
        
        # åˆ†å±‚LSTM
        self.lstm1 = nn.LSTM(
            word_dim, embed_size // 2, 1,
            batch_first=True, bidirectional=True
        )
        self.lstm2 = nn.LSTM(
            embed_size, embed_size // 2, 1,
            batch_first=True, bidirectional=True
        )
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ›
        self.self_attention = nn.MultiheadAttention(
            embed_dim=embed_size,
            num_heads=16,
            dropout=dropout,
            batch_first=True
        )
        
        # åˆ†å±‚æ³¨æ„åŠ› - è¯çº§åˆ«å’Œå¥å­çº§åˆ«
        self.word_attention = nn.Sequential(
            nn.Linear(embed_size, embed_size),
            nn.Tanh(),
            nn.Linear(embed_size, 1)
        )
        
        self.sentence_projection = nn.Sequential(
            nn.Linear(embed_size, embed_size),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_size, embed_size)
        )
        
        self.layer_norm = nn.LayerNorm(embed_size)
        self.dropout = nn.Dropout(dropout)
        
        self.init_weights()
    
    def init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        # è¯åµŒå…¥åˆå§‹åŒ–
        nn.init.uniform_(self.embed.weight, -0.1, 0.1)
        
        # LSTMåˆå§‹åŒ–
        for lstm in [self.lstm1, self.lstm2]:
            for name, param in lstm.named_parameters():
                if 'weight_ih' in name:
                    nn.init.xavier_uniform_(param.data)
                elif 'weight_hh' in name:
                    nn.init.orthogonal_(param.data)
                elif 'bias' in name:
                    param.data.fill_(0)
    
    def forward(self, captions, lengths):
        batch_size, max_len = captions.size()
        
        # è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        embedded = self.embed(captions)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        pos_len = min(max_len, self.pos_embedding.size(0))
        embedded[:, :pos_len] += self.pos_embedding[:pos_len].unsqueeze(0)
        
        embedded = self.embed_dropout(embedded)
        
        # åˆ†å±‚LSTM
        packed1 = nn.utils.rnn.pack_padded_sequence(
            embedded, lengths, batch_first=True, enforce_sorted=False
        )
        lstm_out1, _ = self.lstm1(packed1)
        lstm_out1, _ = nn.utils.rnn.pad_packed_sequence(lstm_out1, batch_first=True)
        
        packed2 = nn.utils.rnn.pack_padded_sequence(
            lstm_out1, lengths, batch_first=True, enforce_sorted=False
        )
        lstm_out2, _ = self.lstm2(packed2)
        lstm_out2, _ = nn.utils.rnn.pad_packed_sequence(lstm_out2, batch_first=True)
        
        # è‡ªæ³¨æ„åŠ›
        attn_mask = torch.zeros(batch_size, max_len, dtype=torch.bool)
        for i, length in enumerate(lengths):
            attn_mask[i, length:] = True
        
        if lstm_out2.is_cuda:
            attn_mask = attn_mask.cuda()
        
        attn_out, _ = self.self_attention(
            lstm_out2, lstm_out2, lstm_out2, 
            key_padding_mask=attn_mask
        )
        
        # æ®‹å·®è¿æ¥
        lstm_out2 = self.layer_norm(lstm_out2 + attn_out)
        
        # è¯çº§æ³¨æ„åŠ›
        attention_weights = self.word_attention(lstm_out2).squeeze(-1)
        
        # åº”ç”¨mask
        mask = torch.zeros(batch_size, max_len)
        for i, length in enumerate(lengths):
            mask[i, :length] = 1
        
        if lstm_out2.is_cuda:
            mask = mask.cuda()
        
        attention_weights.masked_fill_(mask == 0, -1e9)
        attention_weights = F.softmax(attention_weights, dim=1)
        
        # åŠ æƒæ±‚å’Œ
        features = torch.bmm(attention_weights.unsqueeze(1), lstm_out2).squeeze(1)
        
        # å¥å­çº§æŠ•å½±
        features = self.sentence_projection(features)
        features = self.dropout(features)
        
        # L2æ ‡å‡†åŒ–
        features = F.normalize(features, p=2, dim=1)
        
        return features


class AdvancedLosses(nn.Module):
    """é«˜çº§æŸå¤±å‡½æ•°é›†åˆ"""
    
    def __init__(self, temperature=0.05, margin=0.15):
        super(AdvancedLosses, self).__init__()
        self.temperature = temperature
        self.margin = margin
    
    def infonce_loss(self, img_emb, cap_emb):
        """InfoNCEæŸå¤±"""
        batch_size = img_emb.size(0)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.mm(img_emb, cap_emb.t()) / self.temperature
        
        # æ ‡ç­¾
        labels = torch.arange(batch_size)
        if img_emb.is_cuda:
            labels = labels.cuda()
        
        # åŒå‘æŸå¤±
        loss_i2t = F.cross_entropy(sim_matrix, labels)
        loss_t2i = F.cross_entropy(sim_matrix.t(), labels)
        
        return (loss_i2t + loss_t2i) / 2
    
    def adaptive_triplet_loss(self, img_emb, cap_emb, epoch, total_epochs):
        """è‡ªé€‚åº”ä¸‰å…ƒç»„æŸå¤±"""
        batch_size = img_emb.size(0)
        
        # è‡ªé€‚åº”ç¡¬è´Ÿä¾‹æ¯”ä¾‹
        progress = epoch / total_epochs
        hard_ratio = 0.2 + 0.6 * progress  # ä»0.2å¢é•¿åˆ°0.8
        
        scores = torch.mm(img_emb, cap_emb.t())
        diagonal = scores.diag().view(-1, 1)
        
        # Image to text
        d1 = diagonal.expand_as(scores)
        cost_s = (self.margin + scores - d1).clamp(min=0)
        
        # Text to image
        d2 = diagonal.t().expand_as(scores)
        cost_im = (self.margin + scores - d2).clamp(min=0)
        
        # æ¸…é™¤å¯¹è§’çº¿
        mask = torch.eye(batch_size) > 0.5
        if scores.is_cuda:
            mask = mask.cuda()
        cost_s = cost_s.masked_fill_(mask, 0)
        cost_im = cost_im.masked_fill_(mask, 0)
        
        # è‡ªé€‚åº”ç¡¬è´Ÿä¾‹é€‰æ‹©
        num_hard = max(1, int(batch_size * hard_ratio))
        cost_s_hard, _ = cost_s.topk(num_hard, dim=1)
        cost_im_hard, _ = cost_im.topk(num_hard, dim=0)
        
        return cost_s_hard.mean() + cost_im_hard.mean()
    
    def consistency_loss(self, features1, features2):
        """ä¸€è‡´æ€§æŸå¤± - ä¸åŒå¢å¼ºç‰ˆæœ¬çš„ä¸€è‡´æ€§"""
        return F.mse_loss(features1, features2)
    
    def diversity_loss(self, features):
        """å¤šæ ·æ€§æŸå¤± - é¿å…ç‰¹å¾é€€åŒ–"""
        batch_size = features.size(0)
        if batch_size < 2:
            return torch.tensor(0.0, device=features.device)
        
        # è®¡ç®—ç‰¹å¾é—´çš„ç›¸ä¼¼åº¦
        normalized_features = F.normalize(features, p=2, dim=1)
        similarity_matrix = torch.mm(normalized_features, normalized_features.t())
        
        # æ’é™¤å¯¹è§’çº¿ï¼Œè®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
        mask = torch.eye(batch_size, device=features.device)
        off_diagonal = similarity_matrix.masked_fill(mask.bool(), 0)
        
        # é¼“åŠ±ç‰¹å¾å¤šæ ·æ€§ï¼ˆé™ä½ç›¸ä¼¼åº¦ï¼‰
        diversity_loss = off_diagonal.abs().mean()
        
        return diversity_loss


class SuperVSEModel(nn.Module):
    """è¶…çº§VSEæ¨¡å‹"""
    
    def __init__(self, opt):
        super(SuperVSEModel, self).__init__()
        
        # è¶…çº§ç¼–ç å™¨
        self.img_enc = SuperImageEncoder(
            img_dim=opt.img_dim,
            embed_size=opt.embed_size,
            num_heads=opt.num_heads,
            num_layers=opt.num_layers,
            hidden_dim=opt.hidden_dim,
            dropout=opt.dropout
        )
        
        self.txt_enc = SuperTextEncoder(
            vocab_size=opt.vocab_size,
            word_dim=opt.word_dim,
            embed_size=opt.embed_size,
            num_layers=opt.num_layers,
            dropout=opt.dropout
        )
        
        # é«˜çº§æŸå¤±å‡½æ•°
        self.losses = AdvancedLosses(
            temperature=opt.temperature,
            margin=opt.margin
        )
        
        # æŒ‡æ•°ç§»åŠ¨å¹³å‡
        self.ema_img_enc = None
        self.ema_txt_enc = None
        self.ema_decay = opt.ema_decay
    
    def update_ema(self):
        """æ›´æ–°EMAæ¨¡å‹"""
        if self.ema_img_enc is None:
            self.ema_img_enc = SuperImageEncoder(
                img_dim=2048, embed_size=self.img_enc.embed_size,
                num_heads=16, num_layers=4,
                hidden_dim=2048, dropout=0.15
            )
            self.ema_txt_enc = SuperTextEncoder(
                vocab_size=1562, word_dim=512,
                embed_size=1024, num_layers=4,
                dropout=0.15
            )
            
            if self.img_enc.region_proj.weight.is_cuda:
                self.ema_img_enc.cuda()
                self.ema_txt_enc.cuda()
            
            # åˆå§‹åŒ–EMAæ¨¡å‹
            for ema_param, param in zip(self.ema_img_enc.parameters(), self.img_enc.parameters()):
                ema_param.data.copy_(param.data)
            for ema_param, param in zip(self.ema_txt_enc.parameters(), self.txt_enc.parameters()):
                ema_param.data.copy_(param.data)
        else:
            # æ›´æ–°EMAå‚æ•°
            for ema_param, param in zip(self.ema_img_enc.parameters(), self.img_enc.parameters()):
                ema_param.data.mul_(self.ema_decay).add_(param.data, alpha=1 - self.ema_decay)
            for ema_param, param in zip(self.ema_txt_enc.parameters(), self.txt_enc.parameters()):
                ema_param.data.mul_(self.ema_decay).add_(param.data, alpha=1 - self.ema_decay)
    
    def forward_emb(self, images, captions, lengths, use_ema=False):
        """å‰å‘ä¼ æ’­å¾—åˆ°åµŒå…¥"""
        if use_ema and self.ema_img_enc is not None:
            img_emb = self.ema_img_enc(images)
            cap_emb = self.ema_txt_enc(captions, lengths)
        else:
            img_emb = self.img_enc(images)
            cap_emb = self.txt_enc(captions, lengths)
        return img_emb, cap_emb
    
    def forward_loss(self, img_emb, cap_emb, epoch, total_epochs):
        """è®¡ç®—ç»¼åˆæŸå¤±"""
        # ä¸»è¦æŸå¤±
        loss_infonce = self.losses.infonce_loss(img_emb, cap_emb)
        loss_triplet = self.losses.adaptive_triplet_loss(img_emb, cap_emb, epoch, total_epochs)
        
        # è¾…åŠ©æŸå¤±
        loss_diversity = self.losses.diversity_loss(torch.cat([img_emb, cap_emb], dim=0))
        
        # ç»„åˆæŸå¤±
        total_loss = (loss_infonce + 
                     0.5 * loss_triplet + 
                     0.2 * loss_diversity)
        
        return total_loss, loss_infonce, loss_triplet, loss_diversity


class SuperTrainer:
    """è¶…çº§è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.opt = SuperOptimizedConfig()
        
        # åŠ è½½è¯æ±‡è¡¨
        self.vocab = self.load_vocab()
        if self.vocab:
            self.opt.vocab_size = len(self.vocab)
        
        print(f"è¯æ±‡è¡¨å¤§å°: {self.opt.vocab_size}")
        
        # è®­ç»ƒç»Ÿè®¡
        self.train_stats = defaultdict(list)
    
    def load_vocab(self):
        """åŠ è½½è¯æ±‡è¡¨"""
        try:
            import pickle
            vocab_path = f'./vocab/{self.opt.data_name}_vocab.pkl'
            if os.path.exists(vocab_path):
                with open(vocab_path, 'rb') as f:
                    return pickle.load(f)
            else:
                print(f"è¯æ±‡è¡¨æ–‡ä»¶ {vocab_path} ä¸å­˜åœ¨")
                return None
        except Exception as e:
            print(f"åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {e}")
            return None
    
    def get_learning_rate(self, epoch, step, total_steps):
        """è·å–å­¦ä¹ ç‡ - ä½™å¼¦é‡å¯"""
        if epoch < self.opt.warmup_epochs:
            # é¢„çƒ­é˜¶æ®µ
            warmup_progress = (epoch * total_steps + step) / (self.opt.warmup_epochs * total_steps)
            return self.opt.lr_warmup + (self.opt.lr_vse - self.opt.lr_warmup) * warmup_progress
        else:
            # ä½™å¼¦é€€ç« + é‡å¯
            adjusted_epoch = epoch - self.opt.warmup_epochs
            cycle_length = self.opt.cosine_restart_epochs
            cycle_epoch = adjusted_epoch % cycle_length
            progress = cycle_epoch / cycle_length
            
            # æ¯æ¬¡é‡å¯åå­¦ä¹ ç‡ç¨å¾®é™ä½
            restart_factor = 0.9 ** (adjusted_epoch // cycle_length)
            
            return self.opt.lr_vse * restart_factor * 0.5 * (1 + math.cos(math.pi * progress))
    
    def mixup_data(self, images, captions, lengths, alpha=0.2):
        """Mixupæ•°æ®å¢å¼º"""
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1
        
        batch_size = images.size(0)
        index = torch.randperm(batch_size)
        if images.is_cuda:
            index = index.cuda()
        
        mixed_images = lam * images + (1 - lam) * images[index]
        
        return mixed_images, index, lam
    
    def train_epoch(self, model, data_loader, optimizer, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        
        batch_time = AverageMeter()
        data_time = AverageMeter()
        train_logger = LogCollector()
        
        end = time.time()
        total_steps = len(data_loader)
        
        for i, train_data in enumerate(data_loader):
            data_time.update(time.time() - end)
            
            # æ•°æ®å‡†å¤‡
            images, captions, lengths, ids = train_data
            
            if torch.cuda.is_available():
                images = images.cuda()
                captions = captions.cuda()
            
            # åŠ¨æ€å­¦ä¹ ç‡
            lr = self.get_learning_rate(epoch, i, total_steps)
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr
            
            # æ•°æ®å¢å¼º
            use_mixup = random.random() < self.opt.augment_prob
            if use_mixup:
                images, mix_index, lam = self.mixup_data(images, captions, lengths, self.opt.mixup_alpha)
            
            # å‰å‘ä¼ æ’­
            img_emb, cap_emb = model.forward_emb(images, captions, lengths)
            
            # è®¡ç®—æŸå¤±
            total_loss, loss_infonce, loss_triplet, loss_diversity = model.forward_loss(
                img_emb, cap_emb, epoch, self.opt.num_epochs
            )
            
            # MixupæŸå¤±è°ƒæ•´
            if use_mixup:
                # å¯¹æ··åˆåçš„ç‰¹å¾è®¡ç®—æŸå¤±ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                total_loss = lam * total_loss + (1 - lam) * total_loss
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), self.opt.grad_clip)
            
            optimizer.step()
            
            # æ›´æ–°EMA
            if epoch >= self.opt.warmup_epochs:
                model.update_ema()
            
            # è®°å½•
            train_logger.update('Loss', total_loss.item(), img_emb.size(0))
            train_logger.update('InfoNCE', loss_infonce.item(), img_emb.size(0))
            train_logger.update('Triplet', loss_triplet.item(), img_emb.size(0))
            train_logger.update('Diversity', loss_diversity.item(), img_emb.size(0))
            
            batch_time.update(time.time() - end)
            end = time.time()
            
            if i % self.opt.log_step == 0:
                print(f'Epoch [{epoch+1}][{i}/{total_steps}] '
                      f'LR: {lr:.6f} '
                      f'Time {batch_time.val:.3f} '
                      f'Loss {train_logger.meters["Loss"].val:.4f} '
                      f'InfoNCE {train_logger.meters["InfoNCE"].val:.4f} '
                      f'Triplet {train_logger.meters["Triplet"].val:.4f} '
                      f'Diversity {train_logger.meters["Diversity"].val:.4f}')
    
    def validate(self, model, data_loader, use_ema=True):
        """æ¨¡å‹éªŒè¯ - å¯é€‰æ‹©ä½¿ç”¨EMAæ¨¡å‹"""
        model.eval()
        
        img_embs = []
        cap_embs = []
        
        print(f"å¼€å§‹éªŒè¯... (ä½¿ç”¨EMA: {use_ema and model.ema_img_enc is not None})")
        with torch.no_grad():
            for val_data in data_loader:
                images, captions, lengths, ids = val_data
                
                if torch.cuda.is_available():
                    images = images.cuda()
                    captions = captions.cuda()
                
                img_emb, cap_emb = model.forward_emb(images, captions, lengths, use_ema=use_ema)
                
                img_embs.append(img_emb.cpu().numpy())
                cap_embs.append(cap_emb.cpu().numpy())
        
        img_embs = np.concatenate(img_embs, axis=0)
        cap_embs = np.concatenate(cap_embs, axis=0)
        
        # è®¡ç®—æ£€ç´¢æŒ‡æ ‡
        (r1, r5, r10, medr, meanr) = i2t(img_embs, cap_embs, measure='cosine')
        (r1i, r5i, r10i, medri, meanri) = t2i(img_embs, cap_embs, measure='cosine')
        
        rsum = r1 + r5 + r10 + r1i + r5i + r10i
        avg_r1 = (r1 + r1i) / 2
        
        print(f"\n=== éªŒè¯ç»“æœ ===")
        print(f"Text to Image: R@1: {r1i:.2f}%, R@5: {r5i:.2f}%, R@10: {r10i:.2f}%")
        print(f"Image to Text: R@1: {r1:.2f}%, R@5: {r5:.2f}%, R@10: {r10:.2f}%")
        print(f"å¹³å‡ R@1: {avg_r1:.2f}%")
        print(f"R-sum: {rsum:.2f}")
        
        return rsum, avg_r1, r1, r1i
    
    def train(self):
        """ä¸»è®­ç»ƒæµç¨‹"""
        print("=== è¶…çº§ä¼˜åŒ–è®­ç»ƒå¼€å§‹ - å†²å‡»70% R@1ç›®æ ‡ ===")
        print(f"ä¸Šä¸€è½®æœ€ä½³ç»“æœ: R@1 = 37.47%, R-sum = 425.07")
        print(f"æœ¬è½®ä¼˜åŒ–ç­–ç•¥: æ›´å¤§æ¨¡å‹+æ·±åº¦ç½‘ç»œ+é«˜çº§æŸå¤±+æ•°æ®å¢å¼º")
        print(f"æ•°æ®è·¯å¾„: {self.opt.data_path}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.opt.batch_size}")
        print(f"åµŒå…¥ç»´åº¦: {self.opt.embed_size}")
        print(f"è¯åµŒå…¥ç»´åº¦: {self.opt.word_dim}")
        print(f"ç½‘ç»œå±‚æ•°: {self.opt.num_layers}")
        print(f"æ³¨æ„åŠ›å¤´æ•°: {self.opt.num_heads}")
        print(f"å­¦ä¹ ç‡: {self.opt.lr_vse}")
        print(f"ç›®æ ‡æ€§èƒ½: R@1 >= {self.opt.target_performance}%")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(self.opt.logger_name, exist_ok=True)
        os.makedirs(self.opt.model_name, exist_ok=True)
        
        # æ•°æ®åŠ è½½å™¨
        if not self.vocab:
            print("è¯æ±‡è¡¨ä¸å­˜åœ¨ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
            return
        
        train_loader = get_precomp_loader(
            self.opt.data_path, 'train', self.vocab, self.opt,
            batch_size=self.opt.batch_size, shuffle=True, 
            num_workers=self.opt.workers
        )
        
        val_loader = get_precomp_loader(
            self.opt.data_path, 'dev', self.vocab, self.opt,
            batch_size=self.opt.batch_size, shuffle=False,
            num_workers=self.opt.workers
        )
        
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_loader.dataset)}")
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_loader.dataset)}")
        
        # æ¨¡å‹
        model = SuperVSEModel(self.opt)
        
        if torch.cuda.is_available():
            model.cuda()
            print("ä½¿ç”¨GPUè®­ç»ƒ")
        else:
            print("ä½¿ç”¨CPUè®­ç»ƒ")
        
        # å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")
        
        # ä¼˜åŒ–å™¨ - ä½¿ç”¨AdamW
        optimizer = optim.AdamW(
            model.parameters(), 
            lr=self.opt.lr_vse,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # è®­ç»ƒçŠ¶æ€
        best_rsum = 0
        best_avg_r1 = 0
        patience_counter = 0
        
        print(f"\nå¼€å§‹è®­ç»ƒ {self.opt.num_epochs} epochs...")
        
        for epoch in range(self.opt.num_epochs):
            print(f"\n{'='*60}")
            print(f"Epoch {epoch+1}/{self.opt.num_epochs}")
            print(f"{'='*60}")
            
            # è®­ç»ƒ
            self.train_epoch(model, train_loader, optimizer, epoch)
            
            # éªŒè¯
            rsum, avg_r1, r1_t2i, r1_i2t = self.validate(model, val_loader, use_ema=True)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = avg_r1 > best_avg_r1
            if is_best:
                best_rsum = rsum
                best_avg_r1 = avg_r1
                patience_counter = 0
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                checkpoint = {
                    'epoch': epoch,
                    'model': model.state_dict(),
                    'best_rsum': best_rsum,
                    'best_avg_r1': best_avg_r1,
                    'r1_t2i': r1_t2i,
                    'r1_i2t': r1_i2t,
                    'opt': self.opt,
                }
                
                model_path = os.path.join(self.opt.model_name, 'best_model.pth')
                torch.save(checkpoint, model_path)
                
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ - R-sum: {best_rsum:.2f}, å¹³å‡R@1: {best_avg_r1:.2f}%")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                if best_avg_r1 >= self.opt.target_performance:
                    print(f"ğŸ‰ğŸ‰ğŸ‰ æˆåŠŸè¾¾åˆ°70%ç›®æ ‡! å¹³å‡R@1: {best_avg_r1:.2f}% >= {self.opt.target_performance}%")
                    print("è®­ç»ƒæå‰ç»“æŸ!")
                    break
                    
            else:
                patience_counter += 1
                
            # æ—©åœæ£€æŸ¥
            if patience_counter >= self.opt.early_stop_patience:
                print(f"æ—©åœè§¦å‘ - è¿ç»­{self.opt.early_stop_patience}ä¸ªepochæ— æ”¹å–„")
                break
            
            # è¿›åº¦æŠ¥å‘Š
            improvement = best_avg_r1 - 37.47  # ç›¸å¯¹äºä¸Šä¸€è½®çš„æ”¹è¿›
            remaining = self.opt.target_performance - best_avg_r1
            
            print(f"å½“å‰æœ€ä½³ - R-sum: {best_rsum:.2f}, å¹³å‡R@1: {best_avg_r1:.2f}%")
            print(f"ç›¸å¯¹ä¸Šè½®æ”¹è¿›: {improvement:.2f}%, è·ç¦»70%ç›®æ ‡: {remaining:.2f}%")
            
            # åŠ¨æ€è°ƒæ•´ç­–ç•¥
            if epoch > 20 and improvement < 1.0:
                print("ğŸ”„ æ€§èƒ½æå‡ç¼“æ…¢ï¼Œè€ƒè™‘è°ƒæ•´è®­ç»ƒç­–ç•¥")
        
        # è®­ç»ƒæ€»ç»“
        print(f"\n{'='*70}")
        print("ğŸ† è¶…çº§ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
        print(f"{'='*70}")
        print(f"åŸºçº¿ç»“æœ: R@1 = 37.47%, R-sum = 425.07")
        print(f"æœ¬è½®ç»“æœ: R@1 = {best_avg_r1:.2f}%, R-sum = {best_rsum:.2f}")
        print(f"æ”¹è¿›ç¨‹åº¦: R@1 {best_avg_r1 - 37.47:+.2f}%, R-sum {best_rsum - 425.07:+.2f}")
        
        if best_avg_r1 >= self.opt.target_performance:
            print("ğŸ‰ æˆåŠŸè¾¾åˆ°70%ç›®æ ‡!")
        else:
            improvement_needed = self.opt.target_performance - best_avg_r1
            print(f"è·ç¦»70%ç›®æ ‡è¿˜å·®: {improvement_needed:.2f}%")
            
            if best_avg_r1 > 50:
                print("ğŸš€ æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼Œç»§ç»­å¾®è°ƒå¯è¾¾åˆ°ç›®æ ‡")
            elif best_avg_r1 > 45:
                print("âš¡ æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            else:
                print("ğŸ”§ éœ€è¦æ›´æ·±å…¥çš„æ¨¡å‹æ”¹è¿›")
                
        print(f"{'='*70}")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    trainer = SuperTrainer()
    trainer.train()


if __name__ == '__main__':
    main() 